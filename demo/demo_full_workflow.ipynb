{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Workflow Demo: Cleaning a Challenging, Messy Dataset\n",
    "\n",
    "This notebook demonstrates the full power of `Transfory` by tackling a messy, real-world dataset. We will use a combination of standard transformers, custom-built transformers, and complex pipelines to clean and prepare the data for machine learning.\n",
    "\n",
    "**The dataset (`challenging_messy_dataset.csv`) contains numerous issues:**\n",
    "- Missing values in multiple columns.\n",
    "- Inconsistent categorical data (e.g., 'm', 'Male', 'M').\n",
    "- Invalid data entries (e.g., ages > 150, impossible dates).\n",
    "- Mixed data types in a single column (e.g., 'Y', 'true', '1').\n",
    "- Extreme outliers.\n",
    "- Unstructured text data in a 'notes' column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add the project root to the path to allow importing 'transfory'\n",
    "project_root = os.path.abspath('..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import all the necessary tools from Transfory\n",
    "from transfory import (\n",
    "    BaseTransformer,\n",
    "    Pipeline,\n",
    "    ColumnTransformer,\n",
    "    MissingValueHandler,\n",
    "    Encoder,\n",
    "    Scaler,\n",
    "    OutlierHandler,\n",
    "    DatetimeFeatureExtractor,\n",
    "    FeatureGenerator,\n",
    "    InsightReporter\n",
    ")\n",
    "\n",
    "print(\"Transfory modules loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the messy dataset\n",
    "df = pd.read_csv('challenging_messy_dataset.csv')\n",
    "\n",
    "print(\"--- Original Data Info ---\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n--- First 5 Rows of Messy Data ---\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating Custom Transformers for Complex Cleaning\n",
    "\n",
    "Some cleaning tasks are too specific for a generic transformer. Here, we'll create a custom transformer to handle the unique messiness in our `gender`, `is_active`, `membership_level`, and `notes` columns. This demonstrates the extensibility of `Transfory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCleaner(BaseTransformer):\n",
    "    \"\"\"A custom transformer to handle specific data inconsistencies.\"\"\"\n",
    "    def __init__(self, name: str = \"CustomCleaner\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def _fit(self, X: pd.DataFrame, y=None):\n",
    "        # This transformer is stateless, so fit does nothing.\n",
    "        pass\n",
    "\n",
    "    def _transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X_out = X.copy()\n",
    "\n",
    "        # 1. Standardize 'gender' column\n",
    "        if 'gender' in X_out.columns:\n",
    "            gender_map = {'m': 'Male', 'M': 'Male', 'f': 'Female', 'F': 'Female'}\n",
    "            X_out['gender'] = X_out['gender'].replace(gender_map)\n",
    "            # Consolidate remaining values into 'Other'\n",
    "            valid_genders = ['Male', 'Female']\n",
    "            X_out['gender'] = X_out['gender'].where(X_out['gender'].isin(valid_genders), 'Other')\n",
    "            self._log(\"transform\", {\"message\": \"Standardized 'gender' column to Male/Female/Other.\"})\n",
    "\n",
    "        # 2. Standardize 'is_active' column to a proper boolean\n",
    "        if 'is_active' in X_out.columns:\n",
    "            active_map = {'Y': True, 'Yes': True, '1': True, 'true': True,\n",
    "                            'N': False, 'No': False, '0': False, 'false': False}\n",
    "            X_out['is_active'] = X_out['is_active'].replace(active_map)\n",
    "            # Any value not in the map (like '?') becomes NaN, to be handled later\n",
    "            X_out['is_active'] = pd.to_numeric(X_out['is_active'], errors='coerce')\n",
    "            self._log(\"transform\", {\"message\": \"Converted 'is_active' to numeric (1/0/NaN).\"})\n",
    "\n",
    "        # 3. Standardize 'membership_level' to lowercase\n",
    "        if 'membership_level' in X_out.columns:\n",
    "            X_out['membership_level'] = X_out['membership_level'].str.lower()\n",
    "            # Replace 'none' string with actual NaN\n",
    "            X_out['membership_level'].replace('none', np.nan, inplace=True)\n",
    "            self._log(\"transform\", {\"message\": \"Normalized 'membership_level' to lowercase.\"})\n",
    "\n",
    "        # 4. Feature Engineering from 'notes' column\n",
    "        if 'notes' in X_out.columns:\n",
    "            X_out['has_complaint'] = X_out['notes'].str.contains('complaint', case=False, na=False).astype(int)\n",
    "            self._log(\"transform\", {\"message\": \"Created 'has_complaint' feature from 'notes'.\"})\n",
    "\n",
    "        return X_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining Sub-Pipelines for Different Data Types\n",
    "\n",
    "We'll create specialized pipelines for handling numeric, datetime, and categorical data separately. This modular approach is a core strength of `Transfory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for cleaning and transforming numeric features\n",
    "numeric_pipeline = Pipeline([\n",
    "    # Cap impossible ages (e.g., > 100) and extreme incomes/purchases\n",
   "    (\"outliers\", OutlierHandler(method='percentile', lower_quantile=0.01, upper_quantile=0.99)),\n",
    "    # Impute any remaining missing values with the median\n",
    "    (\"imputer\", MissingValueHandler(strategy='median')),\n",
    "    # Generate interaction features between numeric columns\n",
    "    (\"feature_gen\", FeatureGenerator(degree=2, include_interactions=True)),\n",
    "    # Scale all numeric features to a common range\n",
    "    (\"scaler\", Scaler(method='zscore'))\n",
    "])\n",
    "\n",
    "# Pipeline for datetime features\n",
    "datetime_pipeline = Pipeline([\n",
    "    # Extract year and month from the join date\n",
    "    (\"date_extractor\", DatetimeFeatureExtractor(features=['year', 'month']))\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features\n",
    "categorical_pipeline = Pipeline([\n",
    "    # Impute missing values with the most frequent category\n",
    "    (\"imputer\", MissingValueHandler(strategy='mode')),\n",
    "    # Convert categories to one-hot encoded format\n",
    "    (\"encoder\", Encoder(method='onehot'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Building the Master Preprocessing Pipeline\n",
    "\n",
    "Now, we'll assemble everything into a single, powerful pipeline. This master pipeline will first run our `CustomCleaner` on the entire dataset. Then, it will use a `ColumnTransformer` to apply our specialized sub-pipelines to the correct columns in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the InsightReporter to capture all events\n",
    "reporter = InsightReporter()\n",
    "\n",
    "# Define the ColumnTransformer to apply different pipelines to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Apply the numeric pipeline to specific numeric columns\n",
    "        (\"numeric_features\", numeric_pipeline, ['age', 'income', 'last_purchase_amount']),\n",
    "        # Apply the datetime pipeline to the join_date column\n",
    "        (\"datetime_features\", datetime_pipeline, ['join_date']),\n",
    "        # Apply the categorical pipeline to the cleaned categorical columns\n",
    "        (\"categorical_features\", categorical_pipeline, ['city', 'gender', 'membership_level'])\n",
    "    ],\n",
    "    remainder='passthrough' # Keep other columns (like 'is_active' and 'has_complaint')\n",
    ")\n",
    "\n",
    "# Define the final, master pipeline\n",
    "master_pipeline = Pipeline([\n",
    "    # Step 1: Run the custom cleaner first to standardize data\n",
    "    (\"custom_cleaner\", CustomCleaner()),\n",
    "    # Step 2: Run the parallel preprocessor\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    # Step 3: Impute any missing values that might have been created (e.g., in 'is_active')\n",
    "    (\"final_imputer\", MissingValueHandler(strategy='mode'))\n",
    "], logging_callback=reporter.get_callback())\n",
    "\n",
    "print(\"Master pipeline created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Executing the Pipeline and Viewing the Results\n",
    "\n",
    "With our entire workflow defined in a single object, we can now apply it to our messy data with one command: `fit_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we don't need for the model\n",
    "df_to_process = df.drop(columns=['customer_id', 'notes'])\n",
    "\n",
    "# Run the entire pipeline!\n",
    "try:\n",
    "    # We handle errors because the dates are intentionally messy\n",
    "    # The DatetimeFeatureExtractor will coerce invalid dates to NaT, which is expected\n",
    "    transformed_df = master_pipeline.fit_transform(df_to_process)\n",
    "\n",
    "    print(\"\\n--- Transformed Data (first 5 rows) ---\")\n",
    "    display(transformed_df.head())\n",
    "\n",
    "    print(\"\\n--- Final Data Info ---\")\n",
    "    transformed_df.info()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during pipeline execution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. The Explainability Report\n",
    "\n",
    "Finally, the most powerful feature of `Transfory`: the `InsightReporter`. Let's review the detailed, human-readable summary of every single action that was performed on our data. The nested structure clearly shows what happened inside the `ColumnTransformer` and its sub-pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*30 + \" INSIGHT REPORT \" + \"=\"*30)\n",
    "print(reporter.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}